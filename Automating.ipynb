{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up selenium driver\n",
    "my_url = 'https://www.kickstarter.com/discover/advanced?ref=discovery_overlay&sort=distance'\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(my_url)\n",
    "driver.implicitly_wait(10)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"projects\"]/div[4]/a')\n",
    "driver.execute_script(\"arguments[0].click();\", element)\n",
    "element.send_keys(Keys.END)\n",
    "#time.sleep(30)\n",
    "element.send_keys(Keys.HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = driver.find_elements_by_css_selector('.soft-black.mb3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for link in links:\n",
    "    pages.append(link.get_attribute('href'))\n",
    "len(pages)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for page in pages:\n",
    "    uClient = uReq(page)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,\"html.parser\")\n",
    "    \n",
    "    title = page_soup.find(\"h2\",{\"type-28 type-24-md soft-black mb1 project-name\"}).text\n",
    "    pledge = page_soup.findAll(\"div\",{\"class\":\"pledge__info\"})\n",
    "    out_path = \"D:\\\\Proneeth\\\\Kickstarter\\\\\"\n",
    "    title = title.replace(\":\",\"\").replace(\" \",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\").replace('\"','').replace('*','').replace('?','').replace('<','').replace('>','').replace('|','')\n",
    "    title = out_path + title + '.xlsx'\n",
    "    \n",
    "    writer = pd.ExcelWriter(title , engine='xlsxwriter')\n",
    "    #looping through all the pledge infos and saving it into a dataframe\n",
    "    headers = [\"Pledge_Amount\", \"Estimated_Delivery_Time\",\"Backer_Count\"]\n",
    "    df = pd.DataFrame(columns = headers)\n",
    "\n",
    "    if pledge[0].find(\"h2\",{\"class\":\"pledge__amount\"}).text == '\\nPledge without a reward\\n':\n",
    "        for info in pledge[1:len(pledge)]:\n",
    "            pledge_amount = info.find(\"span\",\"pledge__currency-conversion\").span.text\n",
    "            estimated_time_info = info.find(\"time\",\"invisible-if-js js-adjust-time\").text\n",
    "            pledge_backer_count = re.search(\"\\d+\",info.find(\"span\",\"pledge__backer-count\").text).group(0)\n",
    "            row = pd.Series([pledge_amount,estimated_time_info,pledge_backer_count],index = headers)\n",
    "            df = df.append(row,ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        for info in pledge:\n",
    "            pledge_amount = info.find(\"span\",\"pledge__currency-conversion\").span.text\n",
    "            estimated_time_info = info.find(\"time\",\"invisible-if-js js-adjust-time\").text\n",
    "            pledge_backer_count = re.search(\"\\d+\",info.find(\"span\",\"pledge__backer-count\").text).group(0)\n",
    "            row = pd.Series([pledge_amount,estimated_time_info,pledge_backer_count],index = headers)\n",
    "            df = df.append(row,ignore_index = True)\n",
    "    \n",
    "    df.to_excel(writer, sheet_name='Pledge Info')\n",
    "    \n",
    "    #grabbing the href for the faq page\n",
    "    faq_page_link = page_soup.find(\"a\",{\"class\":\"js-load-project-content js-load-project-faqs mx3 project-nav__link--faqs tabbed-nav__link type-14\"})[\"href\"]\n",
    "    faq_page_link = \"https://www.kickstarter.com\" + faq_page_link\n",
    "    \n",
    "    #setting up selenium driver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(faq_page_link)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Clicking on the faq buttons\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "    elements = driver.find_elements_by_css_selector('.js-faq-question-toggle.block.p3.no-outline.flex.items-center')\n",
    "    try:\n",
    "        for element in elements:\n",
    "            driver.execute_script(\"arguments[0].click();\", element)\n",
    "        html_faq = driver.page_source\n",
    "        page_soup_faq = soup(html_faq,\"html.parser\")\n",
    "    except NoSuchElementException:\n",
    "        html_faq = driver.page_source\n",
    "        page_soup_faq = soup(html_faq,\"html.parser\")\n",
    "        \n",
    "    #making a faq dataframe\n",
    "    faq_headers = [\"Question\",\"Answer\"]\n",
    "    df_faq = pd.DataFrame(columns = faq_headers)\n",
    "    elements = page_soup_faq.findAll(\"li\",{\"class\":\"js-faq bg-white border mb2 shadow-button radius2px hover-bg-grey-200 js-expanded\"})\n",
    "\n",
    "    for element in elements:\n",
    "        question = element.find(\"a\",{\"class\":\"js-faq-question-toggle block p3 no-outline flex items-center\"}).text\n",
    "        answer =   element.find(\"div\",{\"class\":\"type-14 navy-700 normal\"}).text  \n",
    "        row = pd.Series([question,answer],index = faq_headers)\n",
    "        df_faq = df_faq.append(row,ignore_index = True)\n",
    "            \n",
    "    df_faq.to_excel(writer, sheet_name='FAQs')\n",
    "    \n",
    "    #clicking on updates button\n",
    "    element_updates = driver.find_element_by_xpath('//*[@id=\"updates-emoji\"]')\n",
    "    driver.execute_script(\"arguments[0].click();\", element_updates)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Checking for the see more button\n",
    "\n",
    "    try:\n",
    "        element = driver.find_element_by_xpath('//*[@id=\"project-post-interface\"]/div/div[11]/div/div/div/button')\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        driver.implicitly_wait(10)\n",
    "        time.sleep(3)\n",
    "        html_update = driver.page_source\n",
    "        page_soup_updates = soup(html_update,\"html.parser\")\n",
    "    except NoSuchElementException:\n",
    "        html_update = driver.page_source\n",
    "        page_soup_updates = soup(html_update,\"html.parser\") \n",
    "        \n",
    "        \n",
    "    #checking the number of updates\n",
    "    updates = page_soup_updates.findAll(\"a\",{\"class\":\"truncated-post soft-black block border border-grey-500 hover-border-dark-grey-400\"})\n",
    "    number_of_updates = len(updates)\n",
    "    \n",
    "    #checking the number of updates visible only for backers\n",
    "    backer_updates = page_soup_updates.findAll(\"div\",{\"class\":\"truncated-post soft-black block border border-grey-500 hover-border-dark-grey-400\"})\n",
    "    number_of_backer_updates = len(backer_updates)\n",
    "    \n",
    "    \n",
    "    #making a dataframe for the updates information\n",
    "    updates_headers = [\"Number\",\"Title\",\"Date\",\"Number_of_comments\",\"Number_of_likes\"]\n",
    "    df_updates = pd.DataFrame(columns = updates_headers)\n",
    "\n",
    "    for update in updates:\n",
    "        update_number = update.find(\"span\",{\"class\":\"type-13 soft-black_50 text-uppercase\"}).text\n",
    "        update_title = update.find(\"h2\",{\"class\":\"mb3\"}).text\n",
    "        update_date = update.find(\"span\",{\"class\":\"type-13 soft-black_50 block-md\"}).text\n",
    "\n",
    "        if not update.findAll(\"span\",{\"class\":\"mr4\"})[0].text:\n",
    "            comments = 0\n",
    "        else:    \n",
    "            comments = update.findAll(\"span\",{\"class\":\"mr4\"})[0].text\n",
    "\n",
    "        if not update.findAll(\"span\",{\"class\":\"mr4\"})[1].text:\n",
    "            likes = 0\n",
    "        else:\n",
    "            likes = update.findAll(\"span\",{\"class\":\"mr4\"})[1].text\n",
    "\n",
    "        row = pd.Series([update_number,update_title,update_date,comments,likes],index = updates_headers)\n",
    "        df_updates = df_updates.append(row,ignore_index = True)\n",
    "\n",
    "    for update in backer_updates:\n",
    "        update_number = update.find(\"span\",{\"class\":\"type-13 soft-black_50 text-uppercase\"}).text\n",
    "        update_title = update.find(\"h2\",{\"class\":\"mb3\"}).text\n",
    "        update_date = update.find(\"span\",{\"class\":\"type-13 soft-black_50 block-md\"}).text\n",
    "\n",
    "        if not update.findAll(\"span\",{\"class\":\"mr4\"})[0].text:\n",
    "            comments = 0\n",
    "        else:    \n",
    "            comments = update.findAll(\"span\",{\"class\":\"mr4\"})[0].text\n",
    "\n",
    "        if not update.findAll(\"span\",{\"class\":\"mr4\"})[1].text:\n",
    "            likes = 0\n",
    "        else:\n",
    "            likes = update.findAll(\"span\",{\"class\":\"mr4\"})[1].text\n",
    "\n",
    "        row = pd.Series([update_number,update_title,update_date,comments,likes],index = updates_headers)\n",
    "        df_updates = df_updates.append(row,ignore_index = True)\n",
    "    \n",
    "    df_updates.to_excel(writer, sheet_name='Updates Info')\n",
    "    \n",
    "    #clicking on comments tab\n",
    "    element_comments = driver.find_element_by_xpath('//*[@id=\"comments-emoji\"]')\n",
    "    driver.execute_script(\"arguments[0].click();\", element_comments)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #clicking on show more comments until it shows all\n",
    "    condition = True\n",
    "    while condition:\n",
    "        try:\n",
    "            element = driver.find_element_by_xpath('//*[@id=\"react-project-comments\"]/div/button')\n",
    "            driver.execute_script(\"arguments[0].click();\", element)\n",
    "            driver.implicitly_wait(10)\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            condition = False \n",
    "    \n",
    "    #clicking on show more replies until it shows all\n",
    "    condition = True\n",
    "    while condition:\n",
    "        try:\n",
    "            element = driver.find_element_by_css_selector('.bttn.bttn-medium.bttn-white.flex.w100p.keyboard-focusable')\n",
    "            driver.execute_script(\"arguments[0].click();\", element)\n",
    "            time.sleep(5)\n",
    "            driver.implicitly_wait(10)\n",
    "        except NoSuchElementException:\n",
    "            condition = False \n",
    "\n",
    "    #parsing the comments page\n",
    "    html_comments = driver.page_source\n",
    "    page_soup_comments = soup(html_comments,\"html.parser\")\n",
    "    \n",
    "    page_comments = page_soup_comments.findAll(\"div\",{\"class\":\"border-box relative break-word border border-grey-400 px3 pt3 pb2 o100p transition-all transition-delay-1000 bg-white\"})\n",
    "    total_comments = len(page_comments)\n",
    "    \n",
    "    #making a dataframe for the comments section\n",
    "    comments_headers = [\"UserName\",\"Comment\",\"Status\",\"Time\"]\n",
    "    df_comments = pd.DataFrame(columns = comments_headers)\n",
    "\n",
    "    for comment in page_comments:\n",
    "        user_name = comment.find(\"span\",{\"class\":\"mr2\"}).text\n",
    "        user_comment = comment.find(\"div\",{\"class\":\"w100p\"}).text\n",
    "        if not comment.findAll(\"span\",{\"class\":\"apricot-600 type-14 mr1\"}):\n",
    "            if not comment.findAll(\"span\",{\"class\":\"bg-ksr-green-700 white px1 type-14 mr1\"}):\n",
    "                user_status = \"backer\"\n",
    "            else:\n",
    "                user_status = comment.find(\"span\",{\"class\":\"bg-ksr-green-700 white px1 type-14 mr1\"}).text\n",
    "        else:\n",
    "            user_status = comment.find(\"span\",{\"class\":\"apricot-600 type-14 mr1\"}).text \n",
    "        user_time = comment.find(\"time\",{\"class\":\"block dark-grey-400 type-12\"})[\"title\"]\n",
    "\n",
    "        row = pd.Series([user_name,user_comment,user_status,user_time],index = comments_headers)\n",
    "        df_comments = df_comments.append(row,ignore_index = True)\n",
    "    \n",
    "    df_comments.to_excel(writer, sheet_name='Comments')\n",
    "    \n",
    "    #clicking on community tab\n",
    "    element_community = driver.find_element_by_xpath('//*[@id=\"community-emoji\"]')\n",
    "    driver.execute_script(\"arguments[0].click();\", element_community)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #parsing the community page\n",
    "    html_community = driver.page_source\n",
    "    page_soup_community = soup(html_community,\"html.parser\") \n",
    "    \n",
    "    #making a dataframe for the community top cities section\n",
    "    cities= page_soup_community.find(\"div\",{\"class\":\"community-section__locations_cities\"})\n",
    "\n",
    "    if cities:\n",
    "        cities_headers = [\"City\",\"Backers\"]\n",
    "        df_cities = pd.DataFrame(columns = cities_headers)\n",
    "        cities_list = cities.findAll(\"div\",{\"class\":\"location-list__item js-location-item\"})\n",
    "\n",
    "        for city in cities_list:\n",
    "            city_name = city.find(\"div\",{\"class\":\"primary-text js-location-primary-text\"}).text\n",
    "            backers = city.find(\"div\",{\"class\":\"tertiary-text js-location-tertiary-text\"}).text\n",
    "            row = pd.Series([city_name,backers],index = cities_headers)\n",
    "            df_cities = df_cities.append(row,ignore_index = True)\n",
    "        \n",
    "        df_cities.to_excel(writer, sheet_name='City-wise-backers')\n",
    "            \n",
    "    #making a dataframe for the community top countries section\n",
    "    countries = page_soup_community.find(\"div\",{\"class\":\"community-section__locations_countries\"})\n",
    "\n",
    "    if countries:\n",
    "        country_headers = [\"Country\",\"Backers\"]\n",
    "        df_countries = pd.DataFrame(columns = country_headers)\n",
    "\n",
    "        countries_list = countries.findAll(\"div\",{\"class\":\"location-list__item js-location-item\"})\n",
    "        for country in countries_list:\n",
    "            country_name = country.find(\"div\",{\"class\":\"primary-text js-location-primary-text\"}).text\n",
    "            backers = country.find(\"div\",{\"class\":\"tertiary-text js-location-tertiary-text\"}).text\n",
    "            row = pd.Series([country_name,backers],index = country_headers)\n",
    "            df_countries = df_countries.append(row,ignore_index = True)\n",
    "        \n",
    "        df_countries.to_excel(writer, sheet_name='Country-wise-backers')\n",
    "    \n",
    "    \n",
    "    #extracting new and existing backer count details\n",
    "    \n",
    "    if page_soup_community.find(\"div\",{\"class\":\"new-backers\"}):\n",
    "        new_backers_count = page_soup_community.find(\"div\",{\"class\":\"new-backers\"}).find(\"div\",{\"class\":\"count\"}).text\n",
    "        if page_soup_community.find(\"div\",{\"class\":\"existing-backers\"}):    \n",
    "            returning_backers_count = page_soup_community.find(\"div\",{\"class\":\"existing-backers\"}).find(\"div\",{\"class\":\"count\"}).text\n",
    "        data = {'New Backers':[new_backers_count],'Returning Backers':[returning_backers_count]}\n",
    "        df_new_and_old = pd.DataFrame(data,columns = ['New Backers','Returning Backers'])\n",
    "        df_new_and_old.to_excel(writer, sheet_name='New and Returning backers')\n",
    "    \n",
    "    driver.quit()\n",
    "    writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
